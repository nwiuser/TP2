‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        FICHIER ACCESS.LOG - EXERCICE NEWSHUB MEDIA
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
---------------------------------
CARACT√âRISTIQUES DU FICHIER
---------------------------------
Nom du fichier    : access.log
Nombre de lignes  : 500 000
Taille            : ~105 MB
Format            : Apache Combined Log Format
P√©riode couverte  : 30 derniers jours
Encodage          : UTF-8


---------------------------------
 FORMAT DES LOGS APACHE
---------------------------------

Chaque ligne suit le format :
IP - - [Date] "M√©thode URL Protocole" Code_HTTP Taille Referrer User-Agent

Exemple :
66.249.66.1 - - [16/Oct/2025:15:15:55 +0000] "GET /article/news-2025-10-0045 HTTP/1.1" 200 23146 "-" "Mozilla/5.0 (compatible; Googlebot/2.1)"

---------------------------------
COMPOSITION DU TRAFIC
---------------------------------

‚Ä¢ Googlebot     : ~35% (175 000 requ√™tes)
‚Ä¢ Visiteurs     : ~60% (300 000 requ√™tes)
‚Ä¢ Bingbot       : ~5%  (25 000 requ√™tes)

---------------------------------
TYPES DE PAGES CRAWL√âES
---------------------------------

1. Articles r√©cents       : /article/news-YYYY-MM-NNNN
   - Ann√©es : 2024, 2025
   - ~30% du trafic

2. Articles archiv√©s      : /archive/YYYY/article-NNNN
   - Ann√©es : 2020-2022
   - ~15% du trafic
   - ATTENTION : Certaines pages retournent 404

3. Pages de pagination    : /category/{tech|sport|politique}?page=N
   - Jusqu'√† 50 pages par cat√©gorie
   - ~20% du trafic
   - PROBL√àME : Consomme beaucoup de crawl budget

4. Page d'accueil         : /, /index.html, /home
   - ~10% du trafic

5. URLs obsol√®tes (404)   : /old-article-N, /deleted-page-N
   - ~8% du trafic
   - PROBL√àME : Gaspillage de crawl budget

6. Redirections (301)     : /old-url-N
   - ~7% du trafic

7. Fichiers statiques     : /css/*, /js/*, /robots.txt, /sitemap.xml
   - ~10% du trafic

---------------------------------
CODES HTTP
---------------------------------

200 (OK)                : ~70% des requ√™tes
301 (Redirect permanent): ~10%
302 (Redirect temporaire): ~3%
304 (Not Modified)      : ~5%
404 (Not Found)         : ~10% ‚ö†Ô∏è PROBL√àME
500 (Server Error)      : ~2%

---------------------------------
ADRESSES IP GOOGLEBOT (authentiques)
---------------------------------

66.249.66.x
66.249.79.x
66.249.64.x

Identification : User-Agent contient "Googlebot"


üö® PROBL√àMES √Ä IDENTIFIER DANS L'EXERCICE
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

1. ‚ùå Taux d'erreur 404 √©lev√© (~10%)
   ‚Üí Impact : Googlebot perd du temps sur des pages inexistantes

2. ‚ùå Crawl excessif sur pagination
   ‚Üí Impact : Budget crawl gaspill√© sur pages de faible valeur

3. ‚ùå Archives anciennes crawl√©es inutilement
   ‚Üí Impact : Ressources serveur consomm√©es pour contenu obsol√®te

4. ‚ùå Redirections 301 multiples
   ‚Üí Impact : Ralentit le crawl, mauvaise UX

5. ‚ùå Pages obsol√®tes toujours accessibles
   ‚Üí Impact : Dilue l'autorit√© du site

---------------------------------
ANALYSES ATTENDUES
---------------------------------

1. Filtrer uniquement les requ√™tes Googlebot
2. Calculer le nombre de crawls par jour/heure
3. Identifier les Top 20 URLs les plus crawl√©es
4. Mesurer le taux d'erreur 404 pour Googlebot
5. Analyser la distribution par profondeur d'URL
6. D√©tecter les patterns de pagination excessive
7. Quantifier le temps perdu sur pages obsol√®tes

---------------------------------
M√âTRIQUES CL√âS √Ä EXTRAIRE
---------------------------------

‚Ä¢ Total crawls Googlebot
‚Ä¢ Crawls/jour (moyenne, min, max)
‚Ä¢ Top 50 URLs crawl√©es
‚Ä¢ % erreurs 404
‚Ä¢ % redirections 301/302
‚Ä¢ R√©partition par type de page
‚Ä¢ Pages obsol√®tes crawl√©es

---------------------------------
COMMANDES UTILES POUR EXPLORATION
---------------------------------

# Compter les lignes
wc -l access.log

# Filtrer Googlebot uniquement
grep -i "googlebot" access.log > googlebot_only.log

# Compter les erreurs 404
grep " 404 " access.log | wc -l

# Top 10 URLs les plus fr√©quentes
awk '{print $7}' access.log | sort | uniq -c | sort -rn | head -10

# R√©partition par code HTTP
awk '{print $9}' access.log | sort | uniq -c | sort -rn

# Extraire les IPs uniques
awk '{print $1}' access.log | sort -u | wc -l


---------------------------------
Etape √† suivre 
---------------------------------
1. Charger le fichier avec Pandas
2. Parser chaque ligne avec regex
3. Filtrer sur User-Agent contenant "googlebot"
4. Cr√©er des visualisations (Matplotlib/Plotly)
5. G√©n√©rer des recommandations bas√©es sur les donn√©es



