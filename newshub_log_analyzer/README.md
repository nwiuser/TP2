# Newsroom Googlebot Crawl Analysis System

Analyse des logs Apache pour optimiser le crawl Googlebot et identifier les opportunitÃ©s d'amÃ©lioration SEO.

## ğŸ¯ Objectifs

- **Analyser les patterns de crawl** de Googlebot sur votre site
- **Identifier les pages problÃ©matiques** (404, 500, pages obsolÃ¨tes)
- **Optimiser le budget de crawl** pour maximiser l'indexation
- **GÃ©nÃ©rer des rapports CSV et dashboards interactifs**

## ğŸ“‹ Features

âœ… **Parsing Apache Combined Log Format**
- Extraction de: IP, date/heure, mÃ©thode, URL, protocole, code HTTP, User-Agent
- DÃ©tection Googlebot automatique
- Gestion des erreurs de parsing

âœ… **Analyse Approfondie**
- Distribution temporelle (par jour/heure)
- Top URLs crawlÃ©es par Googlebot
- Taux d'erreurs (4xx/5xx) par URL
- Distribution de la profondeur des URLs
- DÃ©tection des pages obsolÃ¨tes

âœ… **Rapports & Visualisations**
- Export CSV dÃ©taillÃ© (crawl_report.csv)
- Dashboard Plotly interactif (HTML)
- 5 visualisations principales

## ğŸš€ Installation

### PrÃ©requis
- Python 3.8+
- pip (gestionnaire de paquets Python)

### Setup

```bash
# 1. Cloner/naviguer vers le dossier
cd newshub_log_analyzer

# 2. (Optionnel) CrÃ©er un environnement virtuel
python -m venv env
env\Scripts\activate  # Windows
source env/bin/activate  # Linux/Mac

# 3. Installer les dÃ©pendances
pip install pandas plotly

# Alternative (si plotly existe dÃ©jÃ ):
pip install plotly  # Pour l'HTML interactif
pip install kaleido  # Pour export PNG (optionnel)
```

## ğŸ“Š Usage

### 1. Analyse Simple avec Sample Logs

```bash
python test_sample.py
```

GÃ©nÃ¨re:
- `reports/crawl_report.csv` - Rapport dÃ©taillÃ© par URL
- `reports/dashboard.html` - Dashboard interactif

### 2. Analyse PersonnalisÃ©e

```python
from log_analyzer import LogAnalyzer
from report_generator import ReportGenerator

# Parser les logs
analyzer = LogAnalyzer('access.log')
df = analyzer.parse_log_file()

# GÃ©nÃ©rer les rapports
generator = ReportGenerator(analyzer)
results = generator.generate_full_report()

print(f"CSV: {results['csv']}")
print(f"HTML: {results['html']}")
```

### 3. Statistiques KPI

```python
from log_analyzer import LogAnalyzer

analyzer = LogAnalyzer('access.log')
analyzer.parse_log_file()

# Statistiques gÃ©nÃ©rales
stats = analyzer.get_statistics()
print(f"Total requests: {stats['total_requests']}")
print(f"Googlebot requests: {stats['googlebot_requests']}")
print(f"Error rate: {stats['error_rate']:.1f}%")

# Top URLs
top_urls = analyzer.get_top_urls(20)
print(top_urls)

# Analyse d'erreurs
errors = analyzer.analyze_status_codes()
print(f"4xx errors: {errors['4xx']}")
print(f"5xx errors: {errors['5xx']}")

# Profondeur des URLs
depth = analyzer.analyze_url_depth()
print(f"Average depth: {depth['avg']:.2f}")
```

## ğŸ“ˆ Output Examples

### CSV Report (crawl_report.csv)

| url | crawl_count | status_codes | avg_size | depth | first_crawl | last_crawl | error_count | error_rate | is_obsolete |
|-----|-------------|--------------|----------|-------|------------|------------|------------|-----------|-----------|
| /article/news-001 | 2 | {200: 2} | 5432.0 | 1 | 2025-10-01 10:02:00 | 2025-10-01 10:04:45 | 0 | 0.0 | 0 |
| /archive/old-article | 1 | {200: 1} | 3456.0 | 1 | 2025-10-01 10:01:00 | 2025-10-01 10:01:00 | 0 | 0.0 | 1 |

### Dashboard HTML (dashboard.html)

Le dashboard contient 5 visualisations Plotly interactives:

1. **ğŸ“ˆ Timeline** - Crawls par jour et par heure
2. **ğŸ” Top URLs** - Classement des 20 URLs les plus crawlÃ©es
3. **âš ï¸ Status Codes** - Distribution des codes HTTP (200, 404, 500, etc.)
4. **ğŸ“Š URL Depth** - Histogramme de la profondeur des URLs
5. **ğŸ—‘ï¸ Obsolete Pages** - Scatter plot des pages obsolÃ¨tes vs crawls

Toutes les visualisations sont **interactives** (zoom, hover, legend toggle).

## ğŸ” Log Format Support

Format acceptÃ©: **Apache Combined Log Format**

```
192.168.1.1 - - [01/Jan/2025:12:34:56 +0000] "GET /article/news HTTP/1.1" 200 5432 "-" "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"
```

**Colonnes extraites:**
- `ip` - Adresse IP du client
- `timestamp` - Date et heure au format datetime
- `date` - Date seule
- `hour` - Heure (0-23)
- `method` - GET, POST, etc.
- `url` - URL demandÃ©e
- `protocol` - HTTP/1.1, HTTP/2, etc.
- `status_code` - 200, 404, 500, etc.
- `size` - Taille rÃ©ponse (bytes)
- `referrer` - Referrer header
- `user_agent` - User-Agent string
- `is_googlebot` - BoolÃ©en (True si Googlebot)
- `is_error` - BoolÃ©en (True si 4xx ou 5xx)
- `url_depth` - Profondeur URL (nombre de /)
- `is_obsolete` - BoolÃ©en (True si /archive/, old, deprecated)

## ğŸ“Š Analyses Disponibles

### LogAnalyzer Methods

```python
analyzer.parse_log_file()                    # Parse Apache logs
analyzer.get_statistics()                    # Stats gÃ©nÃ©rales
analyzer.analyze_temporal_distribution()     # Daily/hourly crawls
analyzer.get_top_urls(n=20)                  # Top N URLs
analyzer.analyze_status_codes()              # Error analysis
analyzer.analyze_url_depth()                 # URL depth stats
analyzer.find_obsolete_urls()                # Detect old pages
analyzer.calculate_kpis()                    # Compute KPIs
analyzer.generate_report()                   # Text report
```

### ReportGenerator Methods

```python
generator.export_crawl_report_csv()          # CSV export
generator.create_interactive_dashboard()     # HTML dashboard
generator.generate_full_report()             # All reports
```

## ğŸ¯ Use Cases

### 1. Audit SEO Googlebot
```bash
python -c "
from log_analyzer import LogAnalyzer
a = LogAnalyzer('access.log')
a.parse_log_file()
print(a.generate_report())
"
```

### 2. Monitoring Continu
ExÃ©cuter quotidiennement:
```bash
#!/bin/bash
python generate_reports.py \
  --log-file /var/log/apache2/access.log \
  --output reports/daily_$(date +%Y%m%d).html
```

### 3. DÃ©tection de ProblÃ¨mes
```python
# Trouver les URLs avec + de 50% d'erreurs
high_error_urls = df[df['error_rate'] > 0.5]
print(high_error_urls[['url', 'crawl_count', 'error_rate']])
```

## ğŸ› ï¸ Troubleshooting

### "No module named 'plotly'"
```bash
pip install plotly
# ou utiliser l'environment virtuel:
.\env\Scripts\pip install plotly
```

### "Can't read access.log"
```
VÃ©rifiez:
- Le chemin du fichier est correct
- Les permissions de lecture (chmod 644 access.log)
- L'encodage est UTF-8
```

### "Columns do not exist" Error
Assurez-vous que le log format est Apache Combined:
```
IP - USER [DATE] "METHOD URL PROTOCOL" STATUS SIZE "REFERRER" "USER-AGENT"
```

## ğŸ“… Log Retention

Pour optimiser les performances:
- **Petits logs** (< 100MB): Parse complet en secondes
- **Logs moyens** (100MB-1GB): ~30-60 secondes
- **Gros logs** (> 1GB): Utilisez `tail` ou `grep` pour filtrer d'abord

```bash
# Parser seulement Googlebot requests
grep -i googlebot access.log > googlebot.log
python analyze.py googlebot.log
```

## ğŸ“ Output Files

Les rapports sont gÃ©nÃ©rÃ©s dans le dossier `reports/`:

```
reports/
â”œâ”€â”€ crawl_report.csv       # Rapport dÃ©taillÃ© (CSV)
â””â”€â”€ dashboard.html         # Dashboard interactif (HTML)
```

## ğŸ” Robots.txt & Ethical Scraping

âš ï¸ **Important**: Ces scripts analysent vos **propres logs**, pas du web scraping.

Bonnes pratiques:
- âœ… Analyser vos propres logs Apache/Nginx
- âœ… Respecter `robots.txt` pour vos rÃ¨gles de crawl
- âœ… Ne pas bloquer les crawlers lÃ©gitimes
- âœ… ImplÃ©menter Crawl-delay si nÃ©cessaire

## ğŸ“– Ressources

- [Apache Log Format Documentation](https://httpd.apache.org/docs/2.4/logs.html)
- [Googlebot User-Agent String](https://support.google.com/webmasters/answer/1061943)
- [Google Search Console - Crawl Stats](https://support.google.com/webmasters/answer/7645953)
- [Plotly Documentation](https://plotly.com/python/)

## ğŸ“œ License

Ce projet est dÃ©veloppÃ© Ã  des fins Ã©ducatives et de rÃ©fÃ©rence.

## ğŸ‘¨â€ğŸ’» Auteur

DÃ©veloppÃ© pour l'analyse des logs Newsroom Googlebot.

---

**Questions?** Consultez les fichiers d'exemple:
- `sample_access.log` - Exemple de donnÃ©es de test
- `test_sample.py` - Script de test complet
