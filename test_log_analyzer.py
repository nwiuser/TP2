"""
test_log_analyzer.py - Test du parseur et analyseur de logs
"""

from log_analyzer import LogAnalyzer
import pandas as pd


def main():
    """Lance l'analyse compl√®te"""
    
    print("\n" + "="*70)
    print("üîç TEST LOG ANALYZER - GOOGLEBOT AUDIT")
    print("="*70)
    
    # Initialiser l'analyseur
    analyzer = LogAnalyzer('access_log.txt')
    
    # Parser le fichier
    df = analyzer.parse_log_file()
    if df is None or df.empty:
        print("‚ùå Erreur: Impossible de parser le fichier")
        return
    
    # Afficher statistiques globales
    print("\n" + "="*70)
    print("üìä STATISTIQUES GLOBALES")
    print("="*70)
    stats = analyzer.get_statistics()
    for key, value in stats.items():
        print(f"  ‚Ä¢ {key:25s}: {value}")
    
    # Analyse temporelle
    print("\n" + "="*70)
    print("üìÖ DISTRIBUTION TEMPORELLE")
    print("="*70)
    by_day, by_hour = analyzer.analyze_temporal_distribution()
    
    if not by_day.empty:
        print("\nüìÜ Crawls par jour (top 10):")
        print(by_day.head(10).to_string(index=False))
    
    if not by_hour.empty:
        print("\n‚è∞ Crawls par heure:")
        print(by_hour.to_string(index=False))
    
    # Top URLs
    print("\n" + "="*70)
    print("üîù TOP 20 URLS CRAWLEES")
    print("="*70)
    top_urls = analyzer.get_top_urls(20)
    if not top_urls.empty:
        for idx, (_, row) in enumerate(top_urls.iterrows(), 1):
            print(f"  {idx:2d}. {row['url'][:60]:60s} -> {row['crawl_count']:5,} crawls")
    
    # Codes HTTP
    print("\n" + "="*70)
    print("‚ö†Ô∏è  ANALYSE DES CODES HTTP")
    print("="*70)
    status_stats = analyzer.analyze_status_codes()
    print(f"\n  Distribution:")
    for code in sorted(status_stats['status_distribution'].keys()):
        count = status_stats['status_distribution'][code]
        pct = round(count / status_stats['total_requests'] * 100, 2)
        print(f"    {code} -> {count:6,} ({pct:5.2f}%)")
    
    # Profondeur
    print("\n" + "="*70)
    print("üìè ANALYSE DE PROFONDEUR")
    print("="*70)
    depth_stats = analyzer.analyze_url_depth()
    print(f"  ‚Ä¢ Profondeur moyenne: {depth_stats['average_depth']}")
    print(f"  ‚Ä¢ Min/Max: {depth_stats['min_depth']}/{depth_stats['max_depth']}")
    print(f"  ‚Ä¢ Plus courante: {depth_stats['most_common_depth']}")
    print(f"\n  Distribution:")
    for depth in sorted(depth_stats['depth_distribution'].keys()):
        count = depth_stats['depth_distribution'][depth]
        print(f"    Niveau {depth}: {count:6,} URLs")
    
    # URLs obsol√®tes
    print("\n" + "="*70)
    print("üóëÔ∏è  URLS OBSOLETES")
    print("="*70)
    obsolete = analyzer.find_obsolete_urls()
    if obsolete:
        for url, count in obsolete[:10]:
            print(f"  ‚Ä¢ {url[:60]:60s} -> {count:,} crawls")
    else:
        print("  ‚úÖ Aucune URL obsol√®te trouv√©e")
    
    # KPIs
    print("\n" + "="*70)
    print("üìà KPIs PRINCIPAUX")
    print("="*70)
    kpis = analyzer.calculate_kpis()
    for key, value in kpis.items():
        if isinstance(value, dict):
            print(f"  ‚Ä¢ {key:30s}: {len(value)} √©l√©ments")
        else:
            print(f"  ‚Ä¢ {key:30s}: {value}")
    
    # Rapport complet
    print("\n" + analyzer.generate_report())
    
    # Exporter les donn√©es
    print("\n" + "="*70)
    print("üíæ EXPORT DES DONNEES")
    print("="*70)
    
    try:
        # CSV des logs Googlebot
        analyzer.googlebot_df.to_csv('googlebot_logs.csv', index=False)
        print("  ‚úÖ googlebot_logs.csv cr√©√©")
        
        # CSV des top URLs
        top_urls.to_csv('top_urls.csv', index=False)
        print("  ‚úÖ top_urls.csv cr√©√©")
        
        # CSV de la distribution temporelle
        by_day.to_csv('crawls_per_day.csv', index=False)
        print("  ‚úÖ crawls_per_day.csv cr√©√©")
    except Exception as e:
        print(f"  ‚ùå Erreur export: {e}")


if __name__ == "__main__":
    main()
